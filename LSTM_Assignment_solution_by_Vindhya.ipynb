{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VindhyaHV/Deep-Learning-Case-Study/blob/main/LSTM_Assignment_solution_by_Vindhya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xyuAUoaor-b"
      },
      "source": [
        "## Assignment : 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFbmqd-qor-f"
      },
      "source": [
        "<pre>\n",
        "1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n",
        "2. Load the data in your notebook.\n",
        "3. After step 2 you have to train 3 types of models as discussed below. \n",
        "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n",
        "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
        "6. You can use any one of the optimizers and choice of Learning rate and momentum.\n",
        "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n",
        "8. Make sure that you are using GPU to train the given models.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCJZtYYQor-h"
      },
      "outputs": [],
      "source": [
        "#you can use gdown modules to import dataset for the assignment\n",
        "#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n",
        "#you can run the below cell to import the required preprocessed data.csv file and glove vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycJiyvador-j"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n",
        "#!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-data-sets/5504/8240/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220906%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220906T124142Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2f9b43f9b2320c0931e805f11630c74b397bc1e68bd024132a0c74e21381ef905915684aa040d4baf5a14072c97e190f2be5a1600d545a78a89d3f1ed440a528eb722e588bfc0cfa87ef226a32aa92eacc5f191e04257f039579991f21c43d179d1dcd2e1982114d84d6f408f467cdce0fac7e728f5eced7029e3c09dcbc63d7c89204ae79e4ede6ff3afafaf0df0bafa34beb167de5522e8f96c736c78cdd19b1d2b788f798399681986fdaedd8948d3a3d7c29b5e82822dd64f6c56c716f5ec6dc0d4e7eb8fec409316a184e66f6bf6bd388c6b4fdca4bf71f8c5f2dfc9cf1a021fd4aacae5fd646be307895089325eba545a3df0c4dbf6831510a45d032cd\" -c -O 'archive.zip'"
      ],
      "metadata": {
        "id": "aroO1xJ6pUH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23TcGaCfor-j"
      },
      "source": [
        "## <font color='red'> Model-1 </font>\n",
        "Build and Train deep neural network as shown below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHhZqxijor-k"
      },
      "source": [
        "<img src='https://i.imgur.com/w395Yk9.png'>\n",
        "ref: https://i.imgur.com/w395Yk9.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBoAYf3tor-k"
      },
      "source": [
        "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
        "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
        "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3fs67tQor-l"
      },
      "source": [
        "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7k3_c-Eor-m"
      },
      "source": [
        "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_pAsdcor-n"
      },
      "source": [
        "# <font color='red'> Model-1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEd1MfLtor-n"
      },
      "outputs": [],
      "source": [
        "# import all the libraries\n",
        "#make sure that you import your libraries from tf.keras and not just keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input,Dense,LSTM,Concatenate,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from  tensorflow.keras.layers import Embedding,Flatten,Dense,Conv1D\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGivfpL1or-o"
      },
      "outputs": [],
      "source": [
        "#read the csv file\n",
        "df = pd.read_csv('preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z5TUyjbor-o"
      },
      "outputs": [],
      "source": [
        "# perform stratified train test split on the dataset\n",
        "y = df['project_is_approved']\n",
        "df.drop(labels = 'project_is_approved',axis=1,inplace=True)\n",
        "X_train , X_test, y_train,y_test = train_test_split(df,y,stratify=y)\n",
        "#X_train , X_cv, y_train,y_cv = train_test_split(X_train,y_train,stratify=y_train)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "#y_cv = to_categorical(y_cv)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwN7H4AMor-o"
      },
      "source": [
        "## 1.1 Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwETKhKMor-o"
      },
      "outputs": [],
      "source": [
        "#since the data is already preprocessed, we can directly move to vectorization part\n",
        "#first we will vectorize the text data\n",
        "#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n",
        "# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
        "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
        "# after text vectorization you should get train_padded_docs and test_padded_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iIhXbc2or-p"
      },
      "outputs": [],
      "source": [
        "#https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train['essay'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Encode training data sentences into sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train['essay'])\n",
        "#cv_sequences = tokenizer.texts_to_sequences(X_cv['essay'])\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test['essay'])\n",
        "\n",
        "# Get max training sequence length\n",
        "maxlen = max([len(x) for x in train_sequences])\n",
        "\n",
        "# Pad the training sequences\n",
        "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "#cv_padded = pad_sequences(cv_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "# Output the results of our work\n",
        "print(\"Word index:\\n\", word_index)\n",
        "print(\"\\nTraining sequences:\\n\", train_sequences)\n",
        "print(\"\\nPadded training sequences:\\n\", train_padded)\n",
        "print(\"\\nPadded training shape:\", train_padded.shape)\n",
        "print(\"Training sequences data type:\", type(train_sequences))\n",
        "print(\"Padded Training sequences data type:\", type(train_padded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1KktWTnor-p"
      },
      "outputs": [],
      "source": [
        "#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n",
        "# we will be storing this data in form of an embedding matrix and will use it while defining our model\n",
        "# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand how to create embedding matrix\n",
        "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4x8DBvbor-q"
      },
      "outputs": [],
      "source": [
        "!unzip 'archive.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('glove.6B.300d.txt','r')\n",
        "embedding_index = dict()\n",
        "for line in f:\n",
        "  word = line.split()[0]\n",
        "  value = np.asarray(line.split()[1:],dtype='float32')\n",
        "  embedding_index[word]=value\n",
        "f.close()\n",
        "print('number of words found:',len(embedding_index))\n",
        "\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "vocab_size = len(word_index)+1\n",
        "embedding_matrix = np.zeros((vocab_size,300))\n",
        "\n",
        "for word,num in word_index.items():\n",
        "  embedding_vector = embedding_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[num] = embedding_vector\n",
        "  \n",
        "print('Embedding matrix shape',embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "5Bz_lARcrKrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XcnTBHIor-q"
      },
      "source": [
        "## 1.2 Categorical feature Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5vb9c-For-r"
      },
      "outputs": [],
      "source": [
        "# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n",
        "# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n",
        "# label encoder gives an error for 'unseen values' (values present in test but not in train)\n",
        "# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n",
        "# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n",
        "# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        "# after categorical feature vectorization you will have column_train_data and column_test_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96vsiNizor-s"
      },
      "outputs": [],
      "source": [
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "enc = OrdinalEncoder(categories='auto', dtype='float64', handle_unknown='use_encoded_value', unknown_value=np.nan)\n",
        "enc.fit(X_train.loc[:,['school_state', 'teacher_prefix', 'project_grade_category','clean_categories','clean_subcategories']])\n",
        "column_train_data = enc.transform(X_train.loc[:,['school_state', 'teacher_prefix','project_grade_category','clean_categories','clean_subcategories']])\n",
        "#column_cv_data = enc.transform(X_cv.loc[:,['school_state', 'teacher_prefix','project_grade_category','clean_categories','clean_subcategories']])\n",
        "column_test_data = enc.transform(X_test.loc[:,['school_state','teacher_prefix','project_grade_category','clean_categories','clean_subcategories']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.loc[:,['school_state', 'teacher_prefix','project_grade_category','clean_categories','clean_subcategories']] = column_train_data\n",
        "#X_cv.loc[:,['school_state', 'teacher_prefix','project_grade_category','clean_categories','clean_subcategories']] = column_cv_data\n",
        "X_test.loc[:,['school_state', 'teacher_prefix','project_grade_category','clean_categories','clean_subcategories']] = column_test_data"
      ],
      "metadata": {
        "id": "jB0chPwum8Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph2XNCK9or-s"
      },
      "source": [
        "## 1.3 Numerical feature Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2sgumPeor-t"
      },
      "outputs": [],
      "source": [
        "# you have to standardise the numerical columns\n",
        "# stack both the numerical features\n",
        "#after numerical feature vectorization you will have numerical_data_train and numerical_data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVYYT7MEor-t"
      },
      "outputs": [],
      "source": [
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scl = StandardScaler()\n",
        "scl.fit(X_train.loc[:,['teacher_number_of_previously_posted_projects','price']])\n",
        "numerical_data_train = scl.transform(X_train.loc[:,['teacher_number_of_previously_posted_projects','price']])\n",
        "#numerical_data_cv = scl.transform(X_cv.loc[:,['teacher_number_of_previously_posted_projects','price']])\n",
        "numerical_data_test = scl.transform(X_test.loc[:,['teacher_number_of_previously_posted_projects','price']])\n",
        "\n",
        "X_train['numerical_data'] = [i.sum() for i in numerical_data_train]\n",
        "#X_cv['numerical_data'] = [i.sum() for i in numerical_data_cv]\n",
        "X_test['numerical_data'] = [i.sum() for i in numerical_data_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2knDuYIor-t"
      },
      "source": [
        "## 1.4 Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jswHdhdEor-u"
      },
      "source": [
        "<img src='https://i.imgur.com/w395Yk9.png'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYx7sw7aor-u"
      },
      "outputs": [],
      "source": [
        "# as of now we have vectorized all our features now we will define our model.\n",
        "# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n",
        "# Please go through - https://keras.io/guides/functional_api/\n",
        "# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n",
        "# while defining your model make sure that you use variable names while defining any length,dimension or size.\n",
        "#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n",
        "# the embedding layer for text data should be non trainable\n",
        "# the embedding layer for categorical data should be trainable\n",
        "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
        "# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n",
        "#print model.summary() after you have defined the model\n",
        "#plot the model using utils.plot_model module and make sure that it is similar to the above image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPtM-_4Kor-u"
      },
      "outputs": [],
      "source": [
        "#https://github.com/mrunal46/Donors-Choose-using-LSTM/blob/master/LSTM%20on%20Donor's%20Choose%20-%20Model%201-Copy1.ipynb\n",
        "dim = 300\n",
        "input_shape_cat = X_train['project_grade_category'].shape[0]\n",
        "\n",
        "#input_seq_total_text_data \n",
        "input_seq_total_text_data = Input(shape=(maxlen,),name='input_seq_total_text_data')\n",
        "embedding_0 = Embedding(vocab_size, dim, input_length=input_shape_cat,weights=[embedding_matrix],name='Emb_Text_Data',trainable=False)(input_seq_total_text_data)\n",
        "lstm = LSTM(128,return_sequences=True)(embedding_0)\n",
        "flatten0 = Flatten()(lstm)\n",
        "\n",
        "#input_school_state\n",
        "\n",
        "input_school_state = Input(shape=(1,),name='input_school_state')\n",
        "embedding_1 = Embedding(51,2,name='Emb_State_Data')(input_school_state)\n",
        "flatten1 = Flatten()(embedding_1)\n",
        "\n",
        "#project_grade_category\n",
        "#input_shape_cat = X_train['project_grade_category'].shape[0]\n",
        "project_grade_category = Input(shape=(1,),name='project_grade_category')\n",
        "embedding_2 = Embedding(4,2,name='Emb_PCG_Data')(project_grade_category)\n",
        "flatten2 = Flatten()(embedding_2)\n",
        "\n",
        "#input_clean_categories\n",
        "#input_shape_cat = X_train['clean_categories'].shape[0]\n",
        "input_clean_categories = Input(shape=(1,),name='input_clean_categories')\n",
        "embedding_3 = Embedding(51,2,name='Emb_clean_categories_Data')(input_clean_categories)\n",
        "flatten3 = Flatten()(embedding_3)\n",
        "\n",
        "#input_clean_subcategories\n",
        "#input_shape_cat = X_train['clean_subcategories'].shape[0]\n",
        "input_clean_subcategories = Input(shape=(1,),name='input_clean_subcategories')\n",
        "embedding_4 = Embedding(387,50,name='Emb_clean_subcategories_Data')(input_clean_subcategories)\n",
        "flatten4 = Flatten()(embedding_4)\n",
        "\n",
        "#input_teacher_prefix \n",
        "#input_shape_cat = X_train['teacher_prefix'].shape[0]\n",
        "input_teacher_prefix = Input(shape=(1,),name='input_teacher_prefix')\n",
        "embedding_5 = Embedding(5,4,name='Emb_teacher_prefix_Data')(input_teacher_prefix)\n",
        "flatten5 = Flatten()(embedding_5)\n",
        "\n",
        "#numeric data\n",
        "#input_shape_num = X_train.loc[:,['teacher_number_of_previously_posted_projects','price']].shape[1]\n",
        "input_num = Input(shape=(1,),name='teacher_number_of_previously_posted_projects_price')\n",
        "Dense0 = Dense(16,name='Dense_for_rem_input')(input_num)\n",
        "\n",
        "#concatenate\n",
        "concatenate = Concatenate(name='concatenate',axis=-1)([flatten0,flatten1,flatten2,flatten3,flatten4,flatten5,Dense0])\n",
        "Dense1 = Dense(16,name='Dense_layer1_after_concat')(concatenate)\n",
        "dropout0 = Dropout(0.5,name='dropout')(Dense1)\n",
        "Dense2 = Dense(16,name='Dense_layer2_after_concat')(dropout0)\n",
        "dropout1 = Dropout(0.75,name='dropout_1')(Dense2)\n",
        "Dense3 = Dense(8,name='Dense_layern_after_concat')(dropout1)\n",
        "outputLayer = Dense(2,activation='softmax',name='output_layer_t_clasify_with_softmax')(Dense3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://keras.io/guides/functional_api/\n",
        "model = Model(inputs = [input_seq_total_text_data,input_school_state,project_grade_category,\n",
        "                        input_clean_categories,input_clean_subcategories,input_teacher_prefix,input_num],outputs=outputLayer)"
      ],
      "metadata": {
        "id": "ehK74ts1vwdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7hsbqqP1-yWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model)"
      ],
      "metadata": {
        "id": "oXoJAO3SARyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIODaUr2or-v"
      },
      "source": [
        "## 1.5 Compiling and fititng your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9ZW3Mo4or-v"
      },
      "outputs": [],
      "source": [
        "#define custom auc as metric , do not use tf.keras.metrics\n",
        "# https://stackoverflow.com/a/46844409 - custom AUC reference 1\n",
        "# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807  - custom AUC reference 2\n",
        "# compile and fit your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GgFIslIor-v"
      },
      "outputs": [],
      "source": [
        "#https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras/46844409#46844409\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.callbacks import Callback\n",
        "class RocCallback(Callback):\n",
        "    def __init__(self,training_data,validation_data):\n",
        "        self.x = training_data[0]\n",
        "        self.y = training_data[1]\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      try:\n",
        "        y_pred_train = self.model.predict_proba(self.x)\n",
        "        roc_train = roc_auc_score(self.y, y_pred_train)\n",
        "        y_pred_val = self.model.predict_proba(self.x_val)\n",
        "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
        "        print('\\rroc-auc_train: %s - roc-auc_val: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
        "        return\n",
        "      except:\n",
        "        print(self.model)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "roc = RocCallback(training_data=(X_train, y_train),\n",
        "                  validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "\n",
        "#tensorboard\n",
        "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)"
      ],
      "metadata": {
        "id": "5_CVSyTlz13n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs = [input_seq_total_text_data,input_school_state,project_grade_category,\n",
        "                        input_clean_categories,input_clean_subcategories,input_teacher_prefix,input_num],outputs=outputLayer)"
      ],
      "metadata": {
        "id": "8LJhx0-V6nY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FRjComkor-w"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "input={'input_seq_total_text_data':train_padded,'input_school_state':X_train['school_state'],'project_grade_category':X_train['project_grade_category'],\n",
        "       'input_clean_categories':X_train['clean_categories'],'input_clean_subcategories':X_train['clean_subcategories'],'input_teacher_prefix':X_train['teacher_prefix'],\n",
        "       'teacher_number_of_previously_posted_projects_price':X_train['numerical_data']}\n",
        "validation={'input_seq_total_text_data':test_padded,'input_school_state':X_test['school_state'],'project_grade_category':X_test['project_grade_category'],\n",
        "       'input_clean_categories':X_test['clean_categories'],'input_clean_subcategories':X_test['clean_subcategories'],'input_teacher_prefix':X_test['teacher_prefix'],\n",
        "       'teacher_number_of_previously_posted_projects_price':X_test['numerical_data']}\n",
        "model.fit(input,y_train,validation_data=(validation, y_test),epochs=10,callbacks=[roc,tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fits "
      ],
      "metadata": {
        "id": "hDXNkkfDhwCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M8A56lMor-x"
      },
      "source": [
        "# <font color='red'> Model-2 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynBihsZor-x"
      },
      "source": [
        "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0NJOxJ3or-y"
      },
      "source": [
        "<pre>\n",
        "1. Fit TF-IDF vectorizer on the Train data <br>\n",
        "2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n",
        "\n",
        "3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \n",
        "frequent words and very very rare words don't give much information.\n",
        "Hint - A preferable IDF range is 2-11 for model 2. <br>\n",
        "4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\n",
        "sentence of train and test data and include only those features(words) which are present in the defined IDF range.\n",
        "5. Perform tokenization on the modified text data same as you have done for previous model.\n",
        "6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n",
        "7. Define the model, compile and fit the model.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zaO7UO6or-z"
      },
      "outputs": [],
      "source": [
        "#https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "text_data = vectorizer.fit_transform(X_train['essay'])\n",
        "word_idf_dict = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))\n",
        "word_idf_dict\n",
        "filtered_word = {}\n",
        "for i,j in word_idf_dict.items():\n",
        "  if j<=11 and j>=2:\n",
        "    filtered_word[i]=j\n",
        "X_train.essay = X_train.essay.apply(lambda row:' '.join([i for i in row.split() if i in filtered_word.keys()]))\n",
        "X_test.essay = X_test.essay.apply(lambda row:' '.join([i for i in row.split() if i in filtered_word.keys()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r0gpbVTor-z"
      },
      "outputs": [],
      "source": [
        "#https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train['essay'])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Encode training data sentences into sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train['essay'])\n",
        "#cv_sequences = tokenizer.texts_to_sequences(X_cv['essay'])\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test['essay'])\n",
        "\n",
        "# Get max training sequence length\n",
        "maxlen = max([len(x) for x in train_sequences])\n",
        "\n",
        "# Pad the training sequences\n",
        "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "#cv_padded = pad_sequences(cv_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=maxlen)\n",
        "# Output the results of our work\n",
        "print(\"Word index:\\n\", word_index)\n",
        "print(\"\\nTraining sequences:\\n\", train_sequences)\n",
        "print(\"\\nPadded training sequences:\\n\", train_padded)\n",
        "print(\"\\nPadded training shape:\", train_padded.shape)\n",
        "print(\"Training sequences data type:\", type(train_sequences))\n",
        "print(\"Padded Training sequences data type:\", type(train_padded))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('glove.6B.300d.txt','r')\n",
        "embedding_index = dict()\n",
        "for line in f:\n",
        "  word = line.split()[0]\n",
        "  value = np.asarray(line.split()[1:],dtype='float32')\n",
        "  embedding_index[word]=value\n",
        "f.close()\n",
        "print('number of words found:',len(embedding_index))\n",
        "\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "vocab_size = len(word_index)+1\n",
        "embedding_matrix = np.zeros((vocab_size,300))\n",
        "\n",
        "for word,num in word_index.items():\n",
        "  embedding_vector = embedding_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[num] = embedding_vector\n",
        "  \n",
        "print('Embedding matrix shape',embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "XiInsS9DpB9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/mrunal46/Donors-Choose-using-LSTM/blob/master/LSTM%20on%20Donor's%20Choose%20-%20Model%201-Copy1.ipynb\n",
        "dim = 300\n",
        "input_shape_cat = X_train['project_grade_category'].shape[0]\n",
        "\n",
        "#input_seq_total_text_data \n",
        "input_seq_total_text_data = Input(shape=(maxlen,),name='input_seq_total_text_data')\n",
        "embedding_0 = Embedding(vocab_size, dim, input_length=input_shape_cat,weights=[embedding_matrix],name='Emb_Text_Data',trainable=False)(input_seq_total_text_data)\n",
        "lstm = LSTM(128,return_sequences=True)(embedding_0)\n",
        "flatten0 = Flatten()(lstm)\n",
        "\n",
        "#input_school_state\n",
        "\n",
        "input_school_state = Input(shape=(1,),name='input_school_state')\n",
        "embedding_1 = Embedding(51,2,name='Emb_State_Data')(input_school_state)\n",
        "flatten1 = Flatten()(embedding_1)\n",
        "\n",
        "#project_grade_category\n",
        "#input_shape_cat = X_train['project_grade_category'].shape[0]\n",
        "project_grade_category = Input(shape=(1,),name='project_grade_category')\n",
        "embedding_2 = Embedding(4,2,name='Emb_PCG_Data')(project_grade_category)\n",
        "flatten2 = Flatten()(embedding_2)\n",
        "\n",
        "#input_clean_categories\n",
        "#input_shape_cat = X_train['clean_categories'].shape[0]\n",
        "input_clean_categories = Input(shape=(1,),name='input_clean_categories')\n",
        "embedding_3 = Embedding(51,2,name='Emb_clean_categories_Data')(input_clean_categories)\n",
        "flatten3 = Flatten()(embedding_3)\n",
        "\n",
        "#input_clean_subcategories\n",
        "#input_shape_cat = X_train['clean_subcategories'].shape[0]\n",
        "input_clean_subcategories = Input(shape=(1,),name='input_clean_subcategories')\n",
        "embedding_4 = Embedding(387,50,name='Emb_clean_subcategories_Data')(input_clean_subcategories)\n",
        "flatten4 = Flatten()(embedding_4)\n",
        "\n",
        "#input_teacher_prefix \n",
        "#input_shape_cat = X_train['teacher_prefix'].shape[0]\n",
        "input_teacher_prefix = Input(shape=(1,),name='input_teacher_prefix')\n",
        "embedding_5 = Embedding(5,4,name='Emb_teacher_prefix_Data')(input_teacher_prefix)\n",
        "flatten5 = Flatten()(embedding_5)\n",
        "\n",
        "#numeric data\n",
        "#input_shape_num = X_train.loc[:,['teacher_number_of_previously_posted_projects','price']].shape[1]\n",
        "input_num = Input(shape=(1,),name='teacher_number_of_previously_posted_projects_price')\n",
        "Dense0 = Dense(16,name='Dense_for_rem_input')(input_num)\n",
        "\n",
        "#concatenate\n",
        "concatenate = Concatenate(name='concatenate',axis=-1)([flatten0,flatten1,flatten2,flatten3,flatten4,flatten5,Dense0])\n",
        "Dense1 = Dense(16,name='Dense_layer1_after_concat')(concatenate)\n",
        "dropout0 = Dropout(0.5,name='dropout')(Dense1)\n",
        "Dense2 = Dense(16,name='Dense_layer2_after_concat')(dropout0)\n",
        "dropout1 = Dropout(0.75,name='dropout_1')(Dense2)\n",
        "Dense3 = Dense(8,name='Dense_layern_after_concat')(dropout1)\n",
        "outputLayer = Dense(2,activation='softmax',name='output_layer_t_clasify_with_softmax')(Dense3)"
      ],
      "metadata": {
        "id": "DPPCXkIi5ot5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://keras.io/guides/functional_api/\n",
        "model = Model(inputs = [input_seq_total_text_data,input_school_state,project_grade_category,\n",
        "                        input_clean_categories,input_clean_subcategories,input_teacher_prefix,input_num],outputs=outputLayer)"
      ],
      "metadata": {
        "id": "Tmgju2Gm5ugf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "WOUCzlB95von"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model)"
      ],
      "metadata": {
        "id": "D1zWCD7N5zUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/\n",
        "\n",
        "#tensorboard\n",
        "log_dir = os.path.join(\"logs\",'fits2', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)"
      ],
      "metadata": {
        "id": "jizh03ak7PSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "input={'input_seq_total_text_data':train_padded,'input_school_state':X_train['school_state'],'project_grade_category':X_train['project_grade_category'],\n",
        "       'input_clean_categories':X_train['clean_categories'],'input_clean_subcategories':X_train['clean_subcategories'],'input_teacher_prefix':X_train['teacher_prefix'],\n",
        "       'teacher_number_of_previously_posted_projects_price':X_train['numerical_data']}\n",
        "validation={'input_seq_total_text_data':test_padded,'input_school_state':X_test['school_state'],'project_grade_category':X_test['project_grade_category'],\n",
        "       'input_clean_categories':X_test['clean_categories'],'input_clean_subcategories':X_test['clean_subcategories'],'input_teacher_prefix':X_test['teacher_prefix'],\n",
        "       'teacher_number_of_previously_posted_projects_price':X_test['numerical_data']}\n",
        "model.fit(input,y_train,validation_data=(validation, y_test),epochs=10,callbacks=[roc,tensorboard_callback])"
      ],
      "metadata": {
        "id": "cZCZrfId6nhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fits2"
      ],
      "metadata": {
        "id": "vvWbFDj67WLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxra5HCRor-z"
      },
      "source": [
        "# <font color='red'> Model-3 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLAV4HvHor-0"
      },
      "source": [
        "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
        "ref: https://i.imgur.com/fkQ8nGo.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nnduK36or-0"
      },
      "outputs": [],
      "source": [
        "#in this model you can use the text vectorized data from model1 \n",
        "#for other than text data consider the following steps\n",
        "# you have to perform one hot encoding of categorical features. You can use onehotencoder() or countvectorizer() for the same.\n",
        "# Stack up standardised numerical features and all the one hot encoded categorical features\n",
        "#the input to conv1d layer is 3d, you can convert your 2d data to 3d using np.newaxis\n",
        "# Note - deep learning models won't work with sparse features, you have to convert them to dense features before fitting in the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform stratified train test split on the dataset\n",
        "#y = df['project_is_approved']\n",
        "#df.drop(labels = 'project_is_approved',axis=1,inplace=True)\n",
        "X_train , X_test, y_train,y_test = train_test_split(df,y,stratify=y)\n",
        "#X_train , X_cv, y_train,y_cv = train_test_split(X_train,y_train,stratify=y_train)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "#y_cv = to_categorical(y_cv)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "EI4JAvcZ2nDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat = X_train.loc[:,['school_state', 'teacher_prefix', 'project_grade_category', 'clean_categories',\n",
        "       'clean_subcategories']]\n",
        "test_cat = X_test.loc[:,['school_state', 'teacher_prefix', 'project_grade_category','clean_categories',\n",
        "       'clean_subcategories']]\n",
        "train_num = X_train.loc[:,['teacher_number_of_previously_posted_projects','price']]\n",
        "test_num = X_test.loc[:,['teacher_number_of_previously_posted_projects','price']]"
      ],
      "metadata": {
        "id": "YFpx9j1dyCF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "school_state: CountVectorizer"
      ],
      "metadata": {
        "id": "YUoOCRa55yAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train['school_state'])\n",
        "train_school_state = vectorizer.transform(X_train['school_state']).todense()\n",
        "test_school_state = vectorizer.transform(X_test['school_state']).todense()"
      ],
      "metadata": {
        "id": "jYaXWvq5z3-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "teacher_prefic : CountVectorizer"
      ],
      "metadata": {
        "id": "0jdIVLvZ7PWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train['teacher_prefix'])\n",
        "train_teacher_prefix = vectorizer.transform(X_train['teacher_prefix']).todense()\n",
        "test_teacher_prefix = vectorizer.transform(X_test['teacher_prefix']).todense()"
      ],
      "metadata": {
        "id": "dlZNdz-L5Wj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "project_grade_category: CountVectorizer"
      ],
      "metadata": {
        "id": "TOtBu0Sg71cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train['teacher_prefix'])\n",
        "train_pgc = vectorizer.transform(X_train['teacher_prefix']).todense()\n",
        "test_pgc = vectorizer.transform(X_test['teacher_prefix']).todense()"
      ],
      "metadata": {
        "id": "WO12EpaS7pY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Category : CountVectorizer"
      ],
      "metadata": {
        "id": "xUkfO6yTKTZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train['clean_categories'])\n",
        "train_clean_category = vectorizer.transform(X_train['clean_categories']).todense()\n",
        "test_clean_category = vectorizer.transform(X_test['clean_categories']).todense()"
      ],
      "metadata": {
        "id": "Uxu8x2318FHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean_subcategories : CountVectrizer"
      ],
      "metadata": {
        "id": "M8j-ZvGkM77x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train['clean_subcategories'])\n",
        "train_clean_subcategory = vectorizer.transform(X_train['clean_subcategories']).todense()\n",
        "test_clean_subcategory = vectorizer.transform(X_test['clean_subcategories']).todense()"
      ],
      "metadata": {
        "id": "yupKxE9yKwHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numeric data standardization"
      ],
      "metadata": {
        "id": "hv8RE0zNODxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "teacher_number_of_previously_posted_projects: StandardScaler"
      ],
      "metadata": {
        "id": "UpC7PqoKPD4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stdscaler = StandardScaler()\n",
        "stdscaler.fit(np.array(X_train['teacher_number_of_previously_posted_projects']).reshape(-1,1))\n",
        "train_teacher_projects = stdscaler.transform(np.array(X_train['teacher_number_of_previously_posted_projects']).reshape(-1,1))\n",
        "test_teacher_projects = stdscaler.transform(np.array(X_test['teacher_number_of_previously_posted_projects']).reshape(-1,1))"
      ],
      "metadata": {
        "id": "W7BXGDQlOId4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Price: StandardScaler"
      ],
      "metadata": {
        "id": "P8tn82RiRjLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stdscaler = StandardScaler()\n",
        "stdscaler.fit(np.array(X_train['price']).reshape(-1,1))\n",
        "train_price = stdscaler.transform(np.array(X_train['price']).reshape(-1,1))\n",
        "test_price = stdscaler.transform(np.array(X_test['price']).reshape(-1,1))"
      ],
      "metadata": {
        "id": "ha9xMXPwOTm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 216\n",
        "dim = 300\n",
        "input_seq_total_text_data = Input(shape=(maxlen,),name='input_seq_text_data')\n",
        "emb_text_data = Embedding(vocab_size, dim, input_length=len(train_padded),weights=[embedding_matrix],name='Emb_Text_Data',trainable=False)(input_seq_total_text_data)\n",
        "lstm = LSTM(128,name='lstm')(emb_text_data)\n",
        "flatten = Flatten()(lstm)\n",
        "\n",
        "Other_than_text_data_array = np.hstack([train_school_state,train_teacher_prefix,train_pgc,train_clean_category,train_clean_subcategory,train_price,train_teacher_projects])\n",
        "Other_than_text_data = Input(shape=(102,1),name='Other_than_text_data')\n",
        "Other_than_text_data_array_test = np.hstack([test_school_state,test_teacher_prefix,test_pgc,test_clean_category,test_clean_subcategory,test_price,test_teacher_projects])\n",
        "Other_than_text_data_test = Input(shape=(102,1),name='Other_than_text_data')\n",
        "\n",
        "Other_than_text_data_array = Other_than_text_data_array[:,:,np.newaxis]\n",
        "Other_than_text_data_array_test = Other_than_text_data_array_test[:,:,np.newaxis]\n",
        "\n",
        "Conv1D_1=Conv1D(32,2,activation='relu',input_shape=(102,1),name='Conv1D-1')(Other_than_text_data)\n",
        "Conv1D_n=Conv1D(32,2,activation='relu',input_shape=(102,1),name='Conv1D-n')(Conv1D_1)\n",
        "flatten_1 = Flatten()(Conv1D_n)\n",
        "concatenate = Concatenate(name='concatenate')([flatten,flatten_1])\n",
        "Dense_layer1_after_concat = Dense(16,name='Dense_layer1_after_concat')(concatenate)\n",
        "dropout=Dropout(0.5)(Dense_layer1_after_concat)\n",
        "Dense_layer2_after_concat = Dense(16,name='Dense_layer2_after_concat')(dropout)\n",
        "dropout_1=Dropout(0.5)(Dense_layer2_after_concat)\n",
        "output_layer_to_classify_with_soft_max = Dense(2,activation='softmax')(dropout_1)"
      ],
      "metadata": {
        "id": "unLF9YD1T-1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input_seq_total_text_data,Other_than_text_data],outputs=output_layer_to_classify_with_soft_max)"
      ],
      "metadata": {
        "id": "NTrxQyJGU9o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "SId-FFpxqGlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model)"
      ],
      "metadata": {
        "id": "jaHjS5B3sTmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "\n",
        "#tensorboard\n",
        "log_dir = os.path.join(\"logs\",'fits3', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)"
      ],
      "metadata": {
        "id": "lEKcZlSKa3hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "input={'input_seq_text_data':train_padded,'Other_than_text_data':Other_than_text_data_array}\n",
        "validation={'input_seq_text_data':test_padded,'Other_than_text_data':Other_than_text_data_array_test}\n",
        "model.fit(input,y_train,validation_data=(validation, y_test),epochs=10,callbacks=[roc,tensorboard_callback])"
      ],
      "metadata": {
        "id": "QI3MGHDEj5N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fits3"
      ],
      "metadata": {
        "id": "0TzNqTo7u-xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XS1Vwn6e9HA8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}