{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMgSstPYv0P"
      },
      "source": [
        "# Text Classification:\n",
        "\n",
        "## Data\n",
        "<pre>\n",
        "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
        "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
        "so from document name, you can extract the label for that document.\n",
        "4. Now our problem is to classify all the documents into any one of the class.\n",
        "5. Below we provided count plot of all the labels in our data. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64U9NzWFYv0V"
      },
      "outputs": [],
      "source": [
        "### count plot of all the class labels. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mK4TJOFYv0h"
      },
      "source": [
        "## Assignment:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlqYFVI3Yv0k"
      },
      "source": [
        "#### sample document\n",
        "<pre>\n",
        "<font color='blue'>\n",
        "Subject: A word of advice\n",
        "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
        "\n",
        "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
        ">\n",
        ">I've said 100 times that there is no \"alternative\" that should think you\n",
        ">might have caught on by now.  And there is no \"alternative\", but the point\n",
        ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
        ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
        ">solve them.\n",
        "\n",
        "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
        "those who are doing it.\n",
        "\n",
        "Jim\n",
        "--\n",
        "Have you washed your brain today?\n",
        "</font>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOtyx2EeStBg"
      },
      "outputs": [],
      "source": [
        "sample = \"\"\"Subject: A word of advice\n",
        "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
        "\n",
        "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
        ">\n",
        ">I've said 100 times that there is no \"alternative\" that should think you\n",
        ">might have caught on by now.  And there is no \"alternative\", but the point\n",
        ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
        ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
        ">solve them.\n",
        "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
        "those who are doing it.\n",
        "Jim\n",
        "--\n",
        "Have you washed your brain today?\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghhqlUCwI3De"
      },
      "outputs": [],
      "source": [
        "#https://stackoverflow.com/questions/48660547/how-can-i-extract-gpelocation-using-nltk-ne-chunk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk import Tree\n",
        "import regex as re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "g0hjLQiEBAZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlSaRXihZDuW"
      },
      "outputs": [],
      "source": [
        "class Preprocess():\n",
        "  def __init__(self,phrase):\n",
        "    self.phrase = phrase\n",
        "\n",
        "  def list_string(self,sample):\n",
        "    string = ' '\n",
        "    for i in sample:\n",
        "      string+=' '+i\n",
        "    return string\n",
        "  \n",
        "  def decontracted(self,sample):\n",
        "\n",
        "    # specific\n",
        "    phrase = re.sub(\"won\\\\'t\", \"will not\", sample)\n",
        "    phrase = re.sub(\"can\\\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(\"shouldn\\\\'t\",\"should not\",phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(\"n\\\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(\"\\\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(\"\\\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(\"\\\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(\"\\\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(\"\\\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(\"\\\\'ve\", \" have\",phrase)\n",
        "    phrase = re.sub(\"\\\\'m\", \" am\",phrase)\n",
        "    return phrase\n",
        "  def underscore_remover(self,sample):\n",
        "      string = ' '\n",
        "      for i in sample.split():\n",
        "        if len(i)>4:\n",
        "          if i[0] == '_' and i[-1]=='_':\n",
        "            i=i[1:-1]\n",
        "        if len(i)>3:\n",
        "          if i[1] == '_':\n",
        "            i = i[2:]\n",
        "        if len(i)>4:\n",
        "          if i[2] == '_':\n",
        "            i = i[3:]\n",
        "        i = re.sub('.*_',' ',i)\n",
        "        string+=' '+i\n",
        "      return string\n",
        "  def get_continuous_chunks(self,text, label):\n",
        "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "    prev = None\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for subtree in chunked:\n",
        "        if type(subtree) == Tree and subtree.label() == label:\n",
        "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
        "        if current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "    #print('done')\n",
        "    return continuous_chunk\n",
        "  def lower_case(self,sample):\n",
        "    string = ' '\n",
        "    for i in sample.split():\n",
        "      i = i.lower()\n",
        "      if (len(i)>=15 or len(i)<=2):\n",
        "        continue\n",
        "      string+=' '+i \n",
        "    return string\n",
        "    \n",
        "  def clean(self,sample):\n",
        "    #subject_list = []\n",
        "    mail_domain = []\n",
        "    l = re.sub('\\(.*\\)',' ',sample)\n",
        "    l = self.decontracted(l)\n",
        "    #print(l)\n",
        "    \n",
        "    subject = re.findall('Subject:.*',l)\n",
        "    subject = self.list_string(subject)\n",
        "    subject = re.sub('[^A-Za-z_]',' ',subject)\n",
        "    subject = self.lower_case(subject)\n",
        "    subject = re.sub('subject',' ',subject)\n",
        "    \n",
        "    subject = self.lower_case(subject)\n",
        "    #subject = subject.split()\n",
        "    #subject = subject[1:]\n",
        "    #subject_list.append(subject)\n",
        "    \n",
        "    l = re.sub('Subject:.*\\\\n',' ',l)\n",
        "    l = re.sub('\\\\t',' ',l)\n",
        "    #print(l)\n",
        "    m = re.findall('@\\S+',l)\n",
        "    for i in range(len(m)):\n",
        "      end = re.sub('@',' ',m[i])\n",
        "      end = end.split('.')\n",
        "      mail_domain+=end\n",
        "    mail = ' '\n",
        "    for i in mail_domain:\n",
        "      i = re.sub('[^A-Za-z]',' ',i)\n",
        "      i = re.sub('[\"\\/\\\\:\\-\\?>\\\\t\\\\n]',' ',i)\n",
        "      mail+=' '+i    \n",
        "    \n",
        "    mail = re.sub('[^A-Za-z]',' ',mail)\n",
        "    mail = self.lower_case(mail)\n",
        "   \n",
        "    l = re.sub('From:.*',' ',l)\n",
        "    \n",
        "    l = re.sub('.*writes:.*',' ',l)\n",
        "    l = re.sub('\\S+:',' ',l)\n",
        "    l = re.sub('<.*>',' ',l)\n",
        "    #print(l)\n",
        "    l = re.sub('[\"\\/\\\\:\\-\\?>\\\\t\\\\n]',' ',l)\n",
        "    l = re.sub('\\S+@\\S+',' ',l)\n",
        "# removing person names\n",
        "    names = self.get_continuous_chunks(l,'PERSON')\n",
        "    #print(names)\n",
        "    places = self.get_continuous_chunks(l,'GPE')\n",
        "    #print(places)\n",
        "    #print(l)\n",
        "    #print(names)\n",
        "    #print(places)\n",
        "    for name in names:\n",
        "      name = self.lower_case(name)\n",
        "      name = re.sub('[\\/\\\\:\\-\\?>\\\\t\\\\n]',' ',name)\n",
        "      name = re.sub('[^A-Za-z]',' ',name)\n",
        "      name = self.underscore_remover(name)\n",
        "      \n",
        "      l = re.sub(name,' ',l)\n",
        "   # print(l)\n",
        "    for place in places:\n",
        "      if len(place.split())>1:\n",
        "        place = self.underscore_remover(place)\n",
        "        place = re.sub('[\\/\\\\:\\-\\?>\\\\t\\\\n]',' ',place)\n",
        "        place = self.lower_case(place)\n",
        "        l = re.sub(place,re.sub(' ','_',place),l) \n",
        "    l = re.sub('[0-9]',' ',l)\n",
        "    #print(l)\n",
        "    l = self.underscore_remover(l) #removing works like _word_\n",
        "    l = re.sub('[^A-Za-z_]',' ',l) \n",
        "    l = self.lower_case(l)\n",
        "    #print(l)\n",
        "    return l,subject,mail\n",
        "\n",
        "  \n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAR5HoR1Yv0m"
      },
      "source": [
        "### Preprocessing:\n",
        "<pre>\n",
        "useful links: <a href='http://www.pyregex.com/'>http://www.pyregex.com/</a>\n",
        "\n",
        "<font color='blue'><b>1.</b></font> Find all emails in the document and then get the text after the \"@\". and then split those texts by '.' \n",
        "after that remove the words whose length is less than or equal to 2 and also remove'com' word and then combine those words by space. \n",
        "In one doc, if we have 2 or more mails, get all.\n",
        "<b>Eg:[test@dm1.d.com, test2@dm2.dm3.com]-->[dm1.d.com, dm3.dm4.com]-->[dm1,d,com,dm2,dm3,com]-->[dm1,dm2,dm3]-->\"dm1 dm2 dm3\" </b> \n",
        "append all those into one list/array. ( This will give length of 18828 sentences i.e one list for each of the document). \n",
        "Some sample output was shown below. \n",
        "\n",
        "> In the above sample document there are emails [jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu]\n",
        "\n",
        "preprocessing:\n",
        "[jcopelan@nyx.cs.du.edu, 65882@mimsy.umd.edu, mangoe@cs.umd.edu] ==> [nyx cs du edu mimsy umd edu cs umd edu] ==> \n",
        "[nyx edu mimsy umd edu umd edu]\n",
        "\n",
        "<font color='blue'><b>2.</b></font> Replace all the emails by space in the original text. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIovFDQzYv03"
      },
      "source": [
        "<pre>\n",
        "<font color='blue'><b>3.</b></font> Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove \n",
        "the word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n",
        "<b>Eg: if we have sentance like \"Subject: Re: Gospel Dating @ \\r\\r\\n\" --> You have to get \"Gospel Dating\"</b> \n",
        "Save all this data into another list/array. \n",
        "\n",
        "<font color='blue'><b>4.</b></font> After you store it in the list, Replace those sentances in original text by space.\n",
        "\n",
        "<font color='blue'><b>5.</b></font> Delete all the sentances where sentence starts with <b>\"Write to:\"</b> or <b>\"From:\"</b>.\n",
        "> In the above sample document check the 2nd line, we should remove that\n",
        "\n",
        "<font color='blue'><b>6.</b></font> Delete all the tags like \"< anyword >\"\n",
        "> In the above sample document check the 4nd line, we should remove that \"< 65882@mimsy.umd.edu >\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>7.</b></font> Delete all the data which are present in the brackets. \n",
        "In many text data, we observed that, they maintained the explanation of sentence \n",
        "or translation of sentence to another language in brackets so remove all those.\n",
        "<b>Eg: \"AAIC-The course that gets you HIRED(AAIC - Der Kurs, der Sie anstellt)\" --> \"AAIC-The course that gets you HIRED\"</b>\n",
        "\n",
        "> In the above sample document check the 4nd line, we should remove that \"(Charley Wingate)\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>8.</b></font> Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n",
        "\n",
        "<font color='blue'><b>9.</b></font> Remove all the words which ends with <b>\":\"</b>.\n",
        "<b>Eg: \"Anyword:\"</b>\n",
        "> In the above sample document check the 4nd line, we should remove that \"writes:\"\n",
        "\n",
        "\n",
        "<font color='blue'><b>10.</b></font> Decontractions, replace words like below to full words. \n",
        "please check the donors choose preprocessing for this \n",
        "<b>Eg: can't -> can not, 's -> is, i've -> i have, i'm -> i am, you're -> you are, i'll --> i will </b>\n",
        "\n",
        "<b> There is no order to do point 6 to 10. but you have to get final output correctly</b>\n",
        "\n",
        "<font color='blue'><b>11.</b></font> Do chunking on the text you have after above preprocessing. \n",
        "Text chunking, also referred to as shallow parsing, is a task that \n",
        "follows Part-Of-Speech Tagging and that adds more structure to the sentence.\n",
        "So it combines the some phrases, named entities into single word.\n",
        "So after that combine all those phrases/named entities by separating <b>\"_\"</b>. \n",
        "And remove the phrases/named entities if that is a \"Person\". \n",
        "You can use <b>nltk.ne_chunk</b> to get these. \n",
        "Below we have given one example. please go through it. \n",
        "\n",
        "useful links: \n",
        "<a href='https://www.nltk.org/book/ch07.html'>https://www.nltk.org/book/ch07.html</a>\n",
        "<a href='https://stackoverflow.com/a/31837224/4084039'>https://stackoverflow.com/a/31837224/4084039</a>\n",
        "<a href='http://www.nltk.org/howto/tree.html'>http://www.nltk.org/howto/tree.html</a>\n",
        "<a href='https://stackoverflow.com/a/44294377/4084039'>https://stackoverflow.com/a/44294377/4084039</a>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oBjB9X-nMNu"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV8gzLUjYv0-"
      },
      "source": [
        "<pre>We did chunking for above two lines and then We got one list where each word is mapped to a \n",
        "POS(parts of speech) and also if you see \"New York\" and \"Srikanth Varma\", \n",
        "they got combined and represented as a tree and \"New York\" was referred as \"GPE\" and \"Srikanth Varma\" was referred as \"PERSON\". \n",
        "so now you have to Combine the \"New York\" with <b>\"_\"</b> i.e \"New_York\"\n",
        "and remove the \"Srikanth Varma\" from the above sentence because it is a person.</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpaC-KF3Yv1A"
      },
      "source": [
        "<pre>\n",
        "<font color='blue'><b>13.</b></font> Replace all the digits with space i.e delete all the digits. \n",
        "> In the above sample document, the 6th line have digit 100, so we have to remove that.\n",
        "\n",
        "<font color='blue'><b>14.</b></font> After doing above points, we observed there might be few word's like\n",
        " <b> \"_word_\" (i.e starting and ending with the _), \"_word\" (i.e starting with the _),\n",
        "  \"word_\" (i.e ending with the _)</b> remove the <b>_</b> from these type of words. \n",
        "\n",
        "<font color='blue'><b>15.</b></font>  We also observed some words like <b> \"OneLetter_word\"- eg: d_berlin, \n",
        "\"TwoLetters_word\" - eg: dr_berlin </b>, in these words we remove the \"OneLetter_\" (d_berlin ==> berlin) and \n",
        "\"TwoLetters_\" (de_berlin ==> berlin). i.e remove the words \n",
        "which are length less than or equal to 2 after spliiting those words by \"_\". \n",
        "\n",
        "<font color='blue'><b>16.</b></font> Convert all the words into lower case and lowe case \n",
        "and remove the words which are greater than or equal to 15 or less than or equal to 2.\n",
        "\n",
        "<font color='blue'><b>17.</b></font> replace all the words except \"A-Za-z_\" with space. \n",
        "\n",
        "<font color='blue'><b>18.</b></font> Now You got Preprocessed Text, email, subject. create a dataframe with those. \n",
        "Below are the columns of the df. \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWUeIN1Yv1N"
      },
      "source": [
        "### To get above mentioned data frame --> Try to Write Total Preprocessing steps in One Function Named Preprocess as below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceASjKizYv1U"
      },
      "source": [
        "### Code checking:\n",
        "\n",
        "<font color='red' size=4>\n",
        "After Writing preprocess function. call that functoin with the input text of 'alt.atheism_49960' doc and print the output of the preprocess function\n",
        "<br>\n",
        "This will help us to evaluate faster, based on the output we can suggest you if there are any changes.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVvT7_m9rBdB"
      },
      "outputs": [],
      "source": [
        "with open('/content/alt.atheism_49960.txt', mode=\"r\", encoding=\"utf-8\", errors = 'ignore') as f:\n",
        "  data = f.read()\n",
        "Pre = Preprocess(data)\n",
        "t,s,m = Pre.clean(data)\n",
        "print('text:',t)\n",
        "print('subject:',s)\n",
        "print('mail:',m)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x3og_iaYv1S"
      },
      "source": [
        "### After writing Preprocess function, call the function for each of the document(18828 docs) and then create a dataframe as mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA8dsOdpJSGT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ucJLtWYv1V"
      },
      "source": [
        "### Training The models to Classify: \n",
        "\n",
        "<pre>\n",
        "1. Combine \"preprocessed_text\", \"preprocessed_subject\", \"preprocessed_emails\" into one column. use that column to model. \n",
        "\n",
        "2. Now Split the data into Train and test. use 25% for test also do a stratify split. \n",
        "\n",
        "3. Analyze your text data and pad the sequnce if required. \n",
        "Sequnce length is not restricted, you can use anything of your choice. \n",
        "you need to give the reasoning\n",
        "\n",
        "4. Do Tokenizer i.e convert text into numbers. please be careful while doing it. \n",
        "if you are using tf.keras \"Tokenizer\" API, it removes the <b>\"_\"</b>, but we need that.\n",
        "\n",
        "5. code the model's ( Model-1, Model-2 ) as discussed below \n",
        "and try to optimize that models.  \n",
        "\n",
        "6. For every model use predefined Glove vectors. \n",
        "<b>Don't train any word vectors while Training the model.</b>\n",
        "\n",
        "7. Use \"categorical_crossentropy\" as Loss. \n",
        "\n",
        "8. Use <b>Accuracy and Micro Avgeraged F1 score</b> as your as Key metrics to evaluate your model. \n",
        "\n",
        "9.  Use Tensorboard to plot the loss and Metrics based on the epoches.\n",
        "\n",
        "10. Please save your best model weights in to <b>'best_model_L.h5' ( L = 1 or 2 )</b>. \n",
        "\n",
        "11. You are free to choose any Activation function, learning rate, optimizer.\n",
        "But have to use the same architecture which we are giving below.\n",
        "\n",
        "12. You can add some layer to our architecture but you <b>deletion</b> of layer is not acceptable.\n",
        "\n",
        "13. Try to use <b>Early Stopping</b> technique or any of the callback techniques that you did in the previous assignments.\n",
        "\n",
        "14. For Every model save your model to image ( Plot the model) with shapes \n",
        "and inlcude those images in the notebook markdown cell, \n",
        "upload those imgages to Classroom. You can use \"plot_model\" \n",
        "please refer <a href='https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model'>this</a> if you don't know how to plot the model with shapes. \n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD9b772wWXhw"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!ls\n",
        "!cd /content/drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64LLeRo3mG2e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY1MgUA7Km_d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/documents')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bvx_5HPeJ2h"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data classification"
      ],
      "metadata": {
        "id": "RKg60qaHBJRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV2A9z0bsSKT"
      },
      "outputs": [],
      "source": [
        "# iterate through all file\n",
        "label = []\n",
        "data = []\n",
        "def read_text_file(file_path):\n",
        "  with open(file_path, mode=\"r\", encoding=\"utf-8\", errors = 'ignore') as f:\n",
        "    return f.read()\n",
        "for file in tqdm(os.listdir()):\n",
        "    # Check whether file is in text format or not\n",
        "    if file.startswith(\"talk.region.misc\"):\n",
        "        label.append(0)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"talk.politics.misc\"):\n",
        "        label.append(1)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"talk.politics.mideast\"):\n",
        "        label.append(2)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"talk.politics.guns\"):\n",
        "        label.append(3)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"soc.religion.christian\"):\n",
        "        label.append(4)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"sci.space\"):\n",
        "        label.append(5)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"sci.med\"):\n",
        "        label.append(6)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"sci.electronics\"):\n",
        "        label.append(7)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"sci.crypt\"):\n",
        "        label.append(8)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"rec.sport.hockey\"):\n",
        "        label.append(9)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"rec.sport.baseball\"):\n",
        "        label.append(10)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"rec.motorcycles\"):\n",
        "        label.append(11)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))     \n",
        "    if file.startswith(\"rec.autos\"):\n",
        "        label.append(12)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"misc.forsale\"):\n",
        "        label.append(13)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"comp.windows.x\"):\n",
        "        label.append(14)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"comp.sys.mac.hardware\"):\n",
        "        label.append(15)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"comp.sys.ibm.pc.hardware\"):\n",
        "        label.append(16)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"comp.os.ms-windows.misc\"):\n",
        "        label.append(17)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))\n",
        "    if file.startswith(\"comp.graphics\"):\n",
        "        label.append(18)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))      \n",
        "    if file.startswith(\"alt.atheism\"):\n",
        "        label.append(19)\n",
        "        file_path = f\"{'/content/drive/MyDrive/documents'}/{file}\"\n",
        "        # call read text file function\n",
        "        data.append(read_text_file(file_path))      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6lnTfOZ3DiR"
      },
      "source": [
        "talk.region.misc\n",
        "talk.politics.misc\n",
        "talk.politics.mideast\n",
        "talk.politics.guns\n",
        "soc.religion.christian\n",
        "sci.space\n",
        "sci.med\n",
        "sci.electronics\n",
        "sci.crypt\n",
        "rec.sport.hockey\n",
        "rec.sport.baseball\n",
        "rec.motorcycles\n",
        "rec.autos\n",
        "misc.forsale\n",
        "comp.windows.x\n",
        "comp.sys.mac.hardware\n",
        "comp.sys.ibm.pc.hardware\n",
        "comp.os.ms-windows.misc\n",
        "comp.graphics\n",
        "alt.atheism"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(label)"
      ],
      "metadata": {
        "id": "p15XsgTVVDsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5NJE8TOh74J"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h3E7Zsnx827"
      },
      "outputs": [],
      "source": [
        "preprocessed_subject = []\n",
        "preprocessed_emails = []\n",
        "preprocessed_text = []\n",
        "for j in tqdm(np.arange(len(data))):\n",
        "  p = Preprocess(data[j])\n",
        "  text,subject,mail = p.clean(data[j])\n",
        "  #text_chunk = tree2conlltags(ne_chunk(pos_tag(word_tokenize(text))))\n",
        "  #sub_chunk = tree2conlltags(ne_chunk(pos_tag(word_tokenize(subject))))\n",
        "  #mail_chunk = tree2conlltags(ne_chunk(pos_tag(word_tokenize(mail))))\n",
        "  preprocessed_text.append(text)\n",
        "  preprocessed_subject.append(subject)\n",
        "  preprocessed_emails.append(mail)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRAdN2LLTkDe"
      },
      "outputs": [],
      "source": [
        "preprocessed = pd.DataFrame(preprocessed_text,columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoAtZSuJUYED"
      },
      "outputs": [],
      "source": [
        "preprocessed['subject'] = preprocessed_subject\n",
        "preprocessed['mail'] = preprocessed_emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2u4O-fmc5wn"
      },
      "outputs": [],
      "source": [
        "len(preprocessed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MEoDW6eCFbo"
      },
      "outputs": [],
      "source": [
        "preprocessed_total = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hVDN8SK_1wO"
      },
      "outputs": [],
      "source": [
        "for i in range(len(preprocessed_text)):\n",
        "  preprocessed_total.append(preprocessed_text[i]+preprocessed_subject[i]+preprocessed_emails[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHhGB-Pouf4K"
      },
      "outputs": [],
      "source": [
        "label = np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7lmaCKm7g8-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1P3bEQyVEc4"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "#https://stackoverflow.com/questions/49073673/include-punctuation-in-keras-tokenizer\n",
        "to_exclude = '!\"#$%&()*+-/:;<=>@[\\\\]^`{|}~\\t\\n'\n",
        "max_word = 10000\n",
        "max_length = 1000\n",
        "t = Tokenizer(filters=to_exclude,num_words=max_word)\n",
        "t.fit_on_texts(preprocessed_total)\n",
        "# define documents\n",
        "t.word_index\n",
        "print('words found :',len(t.word_index))\n",
        "vocab_size = len(t.word_index) + 1\n",
        "encoded_docs = t.texts_to_sequences(preprocessed_total)\n",
        "#encoded_test = t.texts_to_sequences(X_test)\n",
        "#print(encoded_docs)\n",
        "\n",
        "\n",
        "padded_docs = pad_sequences(encoded_docs,maxlen = max_length,padding='post')\n",
        "#padded_test = pad_sequences(encoded_test,maxlen = max_length,padding='post')\n",
        "print(padded_docs.shape)\n",
        "#print(padded_test.shape)\n",
        "\n",
        "#labels = tf.keras.utils.to_categorical(label, 20)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2lIePymmU6rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Test Splitting"
      ],
      "metadata": {
        "id": "urc6Q0VeBVyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(padded_docs,label,test_size = 0.25,stratify=labels,random_state=42)\n",
        "Y_train = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "-i1M__qXNX3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "SZGfcajMNj4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJHDt1eobmCf"
      },
      "outputs": [],
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lonwg2FExVk"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!cd /content/\n",
        "os.chdir('/content')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained Glove Model"
      ],
      "metadata": {
        "id": "73C57YNRBZ2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIAmL0VGlIdh"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.array(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g49TkJtAlPPx"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_vEglx5fSJg"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers import Input \n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np # importing numpy for numerical computation\n",
        "from itertools import combinations\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call Backs"
      ],
      "metadata": {
        "id": "TmsAhZl7BfAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fokopKxRhlxY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "class LossHistory(tf.keras.callbacks.Callback):\n",
        "    def __init__(self,validation_data):\n",
        "      self.x_test = validation_data[0]\n",
        "      self.y_test = validation_data[1]\n",
        "    def on_train_begin(self, logs={}):\n",
        "        ## on begin of training, we are creating a instance varible called history\n",
        "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
        "        self.history={'loss': [],'accuracy': [],'val_loss': [],'val_accuracy': [],'auc': []}\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        true_positives=0\n",
        "        ## on end of each epoch, we will get logs and update the self.history dict\n",
        "        loss = logs.get('loss')\n",
        "        if loss is not None:\n",
        "           if np.isnan(loss) or np.isinf(loss):\n",
        "             print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
        "             self.model.stop_training = True \n",
        "        #Terminate if model weights are None      \n",
        "        self.history['loss'].append(loss)\n",
        "        self.history['accuracy'].append(logs.get('accuracy'))\n",
        "        if logs.get('val_loss', -1) != -1:\n",
        "            self.history['val_loss'].append(logs.get('val_loss'))\n",
        "        if logs.get('val_accuracy', -1) != -1:\n",
        "            self.history['val_accuracy'].append(logs.get('val_accuracy'))\n",
        "        # we can get a list of all predicted values at the end of the epoch\n",
        "        # we can use these predicted value and the true values to calculate any custom evaluation score if it is needed for our model\n",
        "        # Here we are taking log of all true positives and then taking average of it\n",
        "        y_pred = self.model.predict(self.x_test)\n",
        "        y_label_pred=np.argmax(y_pred,axis=1)\n",
        "        #print(y_label_pred)\n",
        "       # y_pred_prob = y_pred[:,1]\n",
        "        recall = recall_score(y_test, y_label_pred, average='micro')\n",
        "        precision = precision_score(y_test, y_label_pred, average='micro')\n",
        "      \n",
        "        #a = pd.DataFrame([y_test,y_pred], columns = ('y','y_pred'))\n",
        "        custom_score = (2*recall*precision)/(recall+precision)\n",
        "        #fpr, tpr, thresholds = metrics.roc_curve(y,y_label_pred)\n",
        "\n",
        "        #we can also calcualte predefined metrics such as precison, recall, etc. using callbacks \n",
        "        #auc = metrics.auc(fpr, tpr)\n",
        "        #auc = roc_auc_score(y_test,y_pred_prob)\n",
        "        #self.history['auc'].append(auc)\n",
        "        print(' F1: ',np.round(custom_score,5))\n",
        "            \n",
        "history_own=LossHistory(validation_data=[X_test,y_test])            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7tr6oGst41U"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate Scheduler Function"
      ],
      "metadata": {
        "id": "NGUmwUJZBiEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def changeLearningRate(epoch,lr):\n",
        "    val = history_own.history['val_accuracy']\n",
        "    for i in range(len(val)-1):\n",
        "      if val[i+1] < val[i]:\n",
        "        lr = 0.9*lr\n",
        "    if (epoch+1)%3 == 0 :\n",
        "        lr = 0.95*lr\n",
        "    return lr"
      ],
      "metadata": {
        "id": "FO7_AZuv9hG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "5-GQcwOvBmLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNZL5iLMMYdG"
      },
      "outputs": [],
      "source": [
        "max_length = 1000\n",
        "input_text = Input(shape=(1000,),dtype = 'int32',name = \"input_text\")\n",
        "#print(input_text.shape)\n",
        "Embedding_layer = Embedding(vocab_size,100,input_length=max_length, weights=[embedding_matrix],trainable=False,name=\"Embedding_layer\")(input_text)\n",
        "#print(Embedding_layer.shape)\n",
        "#print(Embedding_layer.shape)\n",
        "conv_1d_with_size_3 = Conv1D(filters=3, kernel_size=2, padding='same', kernel_initializer='normal',activation='relu',name = \"conv_1d_with_size_8\")(Embedding_layer)\n",
        "conv_1d_with_size_4 = Conv1D(filters=4, kernel_size=2, padding='same', kernel_initializer='normal',activation='relu',name = \"conv_1d_with_size_4\")(Embedding_layer)\n",
        "conv_1d_with_size_5 = Conv1D(filters=5, kernel_size=2, padding='same',kernel_initializer='normal', activation='relu',name = \"conv_1d_with_size_2\")(Embedding_layer)\n",
        "concatenated1 = Concatenate(name = \"concatenated1_above_3_conv_layers\")([conv_1d_with_size_3,conv_1d_with_size_4,conv_1d_with_size_5])\n",
        "maxpool_layer1 = MaxPooling1D(pool_size = 3,name = \"MaxPoolLayer1\")(concatenated1)\n",
        "#print(maxpool_layer1.shape)\n",
        "conv_1d_with_size_3_ = Conv1D(filters=3, kernel_size=2, padding='same',kernel_initializer='normal', activation='relu',name = \"conv_1d_with_size_12_\")(maxpool_layer1)\n",
        "conv_1d_with_size_4_ = Conv1D(filters=4, kernel_size=2, padding='same',kernel_initializer='normal' ,activation='relu',name=\"conv_1d_with_size_4_\")(maxpool_layer1)\n",
        "conv_1d_with_size_5_ = Conv1D(filters=5, kernel_size=2, padding='same',kernel_initializer='normal', activation='relu',name = \"conv_1d_with_size_2_\")(maxpool_layer1)\n",
        "concatenated2 = Concatenate(name = \"concatenated2_above_3_conv_layers\")([conv_1d_with_size_3_,conv_1d_with_size_4_,conv_1d_with_size_5_])\n",
        "maxpool_layer2 = MaxPooling1D(pool_size = 2,name = \"MaxPoolLayer2\")(concatenated2)\n",
        "#print(maxpool_layer2.shape)\n",
        "conv_1d_with_size_16_ = Conv1D(filters=16, kernel_size=3, padding='same', activation='relu',name = \"con_with_filter_size_16\")(maxpool_layer2)\n",
        "flatten = Flatten(name=\"Flatten\")(conv_1d_with_size_16_)\n",
        "#print(flatten.shape)\n",
        "dropout = Dropout(0.5,name = \"Dropout\")(flatten)\n",
        "dense1 = Dense(50,name = \"Dense\")(dropout)\n",
        "outputlayer = Dense(20,activation = 'softmax',name = \"outoutLayer\")(dense1)\n",
        "#print(outputlayer.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbP8iCuTGiPQ"
      },
      "outputs": [],
      "source": [
        "#tensorboard\n",
        "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
        "\n",
        "#model \n",
        "\n",
        "model = Model(inputs=input_text,outputs=outputlayer)\n",
        "\n",
        "#callbacks\n",
        "\n",
        "history_own=LossHistory(validation_data=[X_test,y_test])\n",
        "\n",
        "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "lrschedule = LearningRateScheduler(changeLearningRate)\n",
        "#earltstopping\n",
        "earlystop = EarlyStopping(monitor='val_loss',patience=2)\n",
        "#adam optimizer\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1 , momentum=0.1)#callbacks\n",
        "\n",
        "callback_list = [history_own,lrschedule,earlystop,checkpoint,tensorboard_callback]\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 Summary"
      ],
      "metadata": {
        "id": "NvMt-yJtBoxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZmlerRMnEIk"
      },
      "outputs": [],
      "source": [
        "model.summary()\n",
        "plot_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shBE5boB3yeW"
      },
      "source": [
        "i am living in the New York --> [('i', 'NN'), ('am', 'VBP'), ('living', 'VBG'), ('in', 'IN'), ('the', 'DT'), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAIjZa56JSq6"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train,Y_train,epochs=25, validation_data=(X_test,Y_test), batch_size=128, callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9IoQuBuv_Cy"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fits "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0mwdtcvYv1X"
      },
      "source": [
        "### Model-1: Using 1D convolutions with word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXPPsovJ3ePk"
      },
      "source": [
        "<pre>\n",
        "<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \n",
        "In the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n",
        " i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n",
        " we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n",
        "<img src='https://i.imgur.com/kiVQuk1.png'>\n",
        "Ref: https://i.imgur.com/kiVQuk1.png\n",
        "\n",
        "<b>Reference:</b>\n",
        "<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n",
        "<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n",
        "\n",
        "<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n",
        "\n",
        "</pre>\n",
        "\n",
        "### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGVQKge3Yv1e"
      },
      "source": [
        "<img src='https://i.imgur.com/fv1GvFJ.png'>\n",
        "ref: 'https://i.imgur.com/fv1GvFJ.png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6SBG5AYv1f"
      },
      "source": [
        "<pre>\n",
        "1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n",
        "\n",
        "2. use concatenate layer is to concatenate all the filters/channels. \n",
        "\n",
        "3. You can use any pool size and stride for maxpooling layer.\n",
        "\n",
        "4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n",
        "( Only recommendation if you have less computing power )\n",
        "\n",
        "5. You can use any number of layers after the Flatten Layer.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzMR8nGBqDpL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cg4L1V4Yv1d"
      },
      "source": [
        "### Model-2 : Using 1D convolutions with character embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djg4YVA3oQx"
      },
      "source": [
        "<pre>\n",
        "<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\n",
        "Here are the some papers based on Char-CNN\n",
        " 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n",
        " 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n",
        " 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n",
        " 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA4Qkaxs5KlA"
      },
      "outputs": [],
      "source": [
        "#https://towardsdatascience.com/character-level-cnn-with-keras-50391c3adf33\n",
        "to_exclude = '!\"#$%&()*+-/:;<=>@[\\\\]^`{|}~\\t\\n'\n",
        "text = preprocessed_total\n",
        "tk = Tokenizer(filters=to_exclude,num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(preprocessed_total)\n",
        "# define documents\n",
        "print(tk.word_index)\n",
        "#print(t.word_index)\n",
        "vocab_size_ = len(tk.word_index) + 1\n",
        "encoded_docs_ = tk.texts_to_sequences(preprocessed_total)\n",
        "#print(encoded_docs)\n",
        "\n",
        "max_length_ = 29\n",
        "padded_docs_model2 = pad_sequences(encoded_docs_, maxlen=max_length_,padding='post')\n",
        "print(padded_docs_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxkbBXIjtGH4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_,X_test_,y_train_,y_test_ = train_test_split(padded_docs_model2,label,test_size = 0.25,stratify=labels,random_state=42)\n",
        "Y_train_ = tf.keras.utils.to_categorical(y_train)\n",
        "Y_test_ = tf.keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lweMneTPlu_V"
      },
      "outputs": [],
      "source": [
        "alphabet = \"abcdefghijklmnopqrstuvwxyz_\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "RHvr9OA50cqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7p18BNt4eNk"
      },
      "outputs": [],
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.840B.300d-char.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.array(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-N-_Ih5-inj"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size,300))\n",
        "for word, i in tk.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp4CCssIAYcW"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En7h0Au0rpyQ"
      },
      "outputs": [],
      "source": [
        "sent_input = Input(shape=29,name = \"sent_input\")\n",
        "#print(input_text.shape)\n",
        "Embedding_layer = Embedding(vocab_size,300,input_length=max_length, weights=[embedding_matrix],trainable=False,name=\"Embedding_layer\")(sent_input)\n",
        "#print(Embedding_layer.shape)\n",
        "conv_1d_with_size_5 = Conv1D(filters=5, kernel_size=2, padding='same',kernel_initializer='normal',activation='relu',name = \"conv_1d_with_size_8\")(Embedding_layer)\n",
        "conv_1d_with_size_4 = Conv1D(filters=4, kernel_size=2, padding='same', kernel_initializer='normal',activation='relu',name = \"conv_1d_with_size_4\")(conv_1d_with_size_5)\n",
        "#conv_1d_with_size_2 = Conv1D(filters=2, kernel_size=2, padding='same', activation='relu',name = \"conv_1d_with_size_2\")(Embedding_layer)\n",
        "#concatenated1 = Concatenate(name = \"concatenated1_above_3_conv_layers\")([conv_1d_with_size_8,conv_1d_with_size_4,conv_1d_with_size_2])\n",
        "maxpool_layer1 = MaxPooling1D(pool_size = 2,name = \"MaxPoolLayer1\")(conv_1d_with_size_4)\n",
        "#print(maxpool_layer1.shape)\n",
        "conv_1d_with_size_5_ = Conv1D(filters=5, kernel_size=2, padding='same',kernel_initializer='normal', activation='relu',name = \"conv_1d_with_size_12_\")(maxpool_layer1)\n",
        "conv_1d_with_size_4_ = Conv1D(filters=4, kernel_size=2, padding='same', kernel_initializer='normal',activation='relu',name=\"conv_1d_with_size_4_\")(conv_1d_with_size_5_)\n",
        "#conv_1d_with_size_2_ = Conv1D(filters=2, kernel_size=2, padding='same', activation='relu',name = \"conv_1d_with_size_2_\")(maxpool_layer1)\n",
        "#concatenated2 = Concatenate(name = \"concatenated2_above_3_conv_layers\")([conv_1d_with_size_8_,conv_1d_with_size_4_,conv_1d_with_size_2_])\n",
        "maxpool_layer2 = MaxPooling1D(pool_size = 2,name = \"MaxPoolLayer2\")(conv_1d_with_size_4_)\n",
        "#print(maxpool_layer2.shape)\n",
        "#conv_1d_with_size_16_ = Conv1D(filters=16, kernel_size=2, padding='same', activation='relu',name = \"con_with_filter_size_16\")(maxpool_layer2)\n",
        "flatten = Flatten(name=\"Flatten\")(maxpool_layer2)\n",
        "#print(flatten.shape)\n",
        "dropout = Dropout(0.2,name = \"Dropout\")(flatten)\n",
        "dense1 = Dense(100,name = \"Dense\")(dropout)\n",
        "outputlayer = Dense(20,name = \"outoutLayer\",activation='softmax')(dense1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Awmw-9m_hOF"
      },
      "outputs": [],
      "source": [
        "#tensorboard\n",
        "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
        "\n",
        "#model \n",
        "\n",
        "model2 = Model(inputs=sent_input,outputs=outputlayer)\n",
        "\n",
        "#callbacks\n",
        "\n",
        "history_own=LossHistory(validation_data=[X_test_,y_test_])\n",
        "\n",
        "filepath=\"model_save_/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss',  verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "#earltstopping\n",
        "earlystop = EarlyStopping(monitor='val_loss',patience=2)\n",
        "#adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "#callbacks\n",
        "callback_list = [history_own,earlystop,checkpoint,tensorboard_callback]\n",
        "model2.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ryBIKi4_rHo"
      },
      "outputs": [],
      "source": [
        "model2.summary()\n",
        "plot_model(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfqOtPRT_wyo"
      },
      "outputs": [],
      "source": [
        "model2.fit(X_train_,Y_train_,epochs=10, validation_data=(X_test_,Y_test_), batch_size=16, callbacks=callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocyRZI0GAg06"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fits "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXvKSEIeSvN5"
      },
      "source": [
        "<img src='https://i.imgur.com/EuuoJtr.png'>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Text Classification Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}